#!/usr/local/bin/python3

## collapser: Collapses library specific results to genome-level and summarizes them
## Updated: version-1.10 10/24/16
## Property of Meyers Lab at University of Delaware
## author: kakrana@udel.edu

### Description:
### Goes through different file formats generated by Pingchuans script to merge and report
### non-redundant set of phased loci. Also uniq phased loci from each library.
### Contact: atulkakrana@gmail.com

import os,glob,sys,difflib,time,shutil,argparse,operator,datetime,subprocess,multiprocessing
from operator import itemgetter
from multiprocessing import Process, Queue, Pool

########### PHASER DEVELOPER SETTINGS ########

setFile         = "phaser.set"
res_folder      = "summary_%s"   % (datetime.datetime.now().strftime("%m_%d_%H_%M"))      ## Folder with all ther results
cleanup         = 1
cores           = 0

### COLLAPSER DEFAULTS #############
fileType        = 'L'               ## 'L' = *cluster.boundary.without.PARE.validation.list file, N = *NO.by.PARE.file, Y = *YES.by.PARE.file, C: if formatted CSV for intercomparision
region          = 'I'               ## 'G': Genic and 'I': Intergenic - This affects overlap ration in main()

### SUMMARIZER DEFAULTS ############

fetchMax        = 1                                                 ## Fetch the max abundance phasi from tag position summmary, write in separate file
fetchLibAbun    = 0                                                 ## 0: Fetch libwise abundances for tags with 'phase' len required fpr heatmaps and other comparision between loci | 1: All tags from phaster, required to see abundance of phasiRNA tag and tags against all tags in these loci. The latter would be sum of abundances from all libs computed manualy in excel

head            = "Y"
coordsep        ='\t'                                               ## Seprator used in the file
phasedID        = 'N'                                               ## If given than phas ID is used to extract coords, if not than coords specified by user are used
namecol         = 1
pvalcol         = 2
chrcol          = 3                                                 ## As in excel format
startcol        = 4                                                 ## As in excel format
endcol          = 5                                                 ## As in excel format
libcol          = 7

phasiLenFilter  = 'N'                                               ## 'Y' then tags of phase length will be written from the cluster | 'N' - All tags will be written
minAbun         = 1                                                 ## Minimum abundance of tag to be written
matchThres      = 0.25                                              ## Ratio of match required by the cluster to phased loci | For transcripts, to capture biggest cluster like for mapping to IR, use a lower ratio so that longer transcript with smaller match ratio can be included
startbuff       = 0                                                 ## While extracting sequence through coords2FASTA a buffer is added to start, 
                                                                    ## start position in phased ID has this buffer added, ## so minus this buffer 
                                                                    ## for better matching with real loci


#### Command Line ##############################
################################################
parser = argparse.ArgumentParser()

flags = parser.add_argument_group('required arguments') ## Add required arguments to this group and optional to parser

flags.add_argument('-dir',  default='None', type=str, help='directory from your'\
    'phaser run which need to be summarized. Required parameter', required=True)

parser.add_argument('-pval',  default='1e-05', type=float, help='pvalue cutoff to '\
    'filter the phased siRNAs loci or transcipts. Optional parameter', required=False)

# args = parser.parse_args()
args = parser.parse_args()


## If no directory specified
if args.dir == None:
    print("\nPlease specify directory from phaser analysis using the '-dir' parameter")
    print("To see all requred parameters, run: python3 collapser -h\n")
    sys.exit()

## If user add "/" at end of path, this is removed
if args.dir.endswith("/"):
    args.dir = args.dir[0:-1]


#### COLLAPSER FUNCTIONS ########################
#################################################

def readSet(setFile):
    '''
    Read and parse external settings file
    '''

    if os.path.isfile(setFile):
        pass
    else:
        print("---Settings file 'prepro.set' not found in current directory")
        print("---Please copy it to same directory as script and rerun")
        sys.exit()

    print("#### Fn: Settings Reader ####################")
    
    fh_in   = open(setFile, 'r')
    setFile = fh_in.readlines()
    fh_in.close()
    
    for line in setFile:
        if line: ## Not empty
            if line.startswith('@'):
                line = line.strip("\n")
                # print(line)
                akey,aval = line.split('=')
                param = akey.strip()
                value = aval.strip()
                # print(param,value)
                
                ## Extract values ######### 

                if param.strip() == '@runType':
                    global runType
                    runType = str(value.strip())
                    print('User Input runType:              ',runType)

                elif param.strip() == '@index':
                    global index
                    index = str(value.strip())
                    print('User Input index location:       ',index)

                elif param.strip() == '@db':
                    global db
                    db = str(value.strip())
                    print('User Input sRNA DB:              ',db)

                elif param.strip() == '@fetchLib':
                    global fetchLib
                    fetchLib = str(value.strip())
                    print('User Input to auto fetch libs:   ',fetchLib)

                elif param.strip() == '@userLibs':
                    global libs
                    libs = list(map(str,value.strip().split(',')))
                    print('User Input Libs:                 ',libs)

                elif param.strip() == '@genoFile':
                    global genoFile
                    genoFile = str(value.strip())
                    print('User Input genoFile:             ',genoFile)

                elif param.strip() == '@phase':
                    global phase
                    phase = int(value.strip())
                    print('User Input for phase length:     ',phase)
                
                elif param.strip() == '@path_prepro_git':
                    global phaster_path
                    phaster_path = str(value.strip()).rstrip("/")+"/phaster"
                    print('User Input for phaster path:     ',phaster_path) 

            else:
                #print("Missed line:",line)
                pass
    # sys.exit()
    return libs

def pvaluereader():
    '''
    Get the best matching p-value to user cutoff
    '''
    print("\n#### Fn: pvalue capture ###################")

    ### 
    if os.path.isdir(args.dir): ## Check to see its a file from bowtie and not tophat mapped folder - Untested
        # print("Directory from phaser analysis found")
        pass
    else:
        print("Specified directory not found: %s" % (args.dir))
        print("Please confirm the directory path, or")
        print("confirm 'phaser' run finished successfully")
        print("To see all required parameters, run: python3 collapser -h\n")
        print("Script exiting...\n")
        sys.exit()

    ### Read the list result files
    clustL      = [file for file in os.listdir("%s" % (args.dir)) if file.endswith ('.cluster')]
    pvalS       = set()

    if not pvalS:
        print("No *.cluster files found in specified directory:%s" % (args.dir))
        print("pleas check ")

    print("%s clusters cached")
    for aclust in clustL:
        # print(aclust,aclust.rsplit("_",4)[1])
        pval = float(aclust.rsplit("_",4)[1].replace('p',''))
        pvalS.add(pval)

    # print("--P-values cached",pvalS)

    ## Sanity check #######
    if not pvalS:
        print("No 'clusters' found - please confirm 'phaser' run finished successfully")
        sys.exit()
    else:
        pass

    ### Sort pval list ####
    pval_sorted = sorted(pvalS,reverse=True)
    print(pval_sorted)

    ### Find best p-value for ananlysis in if-loop
    for apval in pval_sorted:
        if apval <= float(args.pval):
            pcutoff = float(apval)
            print("--best p-value for analysis:%s" % (pcutoff))
            break
        else:
            pass

    return pcutoff

def prepare(pcutoff,libs,res_folder):

    print("\n#### Fn: prepare ############################")
    
    ### Make a new folder ##############################
    ####################################################
    temp_folder = "./%s/%s" % (res_folder,"temp")
    print("WARNING: 'collapser' results exist from earlier run, these will be deleted")
    shutil.rmtree("%s" % (res_folder),ignore_errors=True)
    os.mkdir("%s" % (res_folder))
    os.mkdir("%s" % (temp_folder))

   ### Get files for collapser #########################
    listL      = [file for file in os.listdir("%s" % (args.dir)) if file.endswith ('.list')]
    print(listL)
    acount = 0 ### Count of library results copied to collapsed folder
    for alistF in listL:
        alib    = alistF.rsplit(".",6)[0]
        # print(alib)
        if alib in libs:
            acount += 1
            afile   = "%s/%s" % (args.dir,alistF)
            shutil.copy(afile,temp_folder)

    ### Get files for summarizer ######################
    ###################################################
    clustL      = [file for file in os.listdir("%s" % (args.dir)) if file.endswith ('.cluster')]
    combL       = []    ## List of file to be combined
    bcount      = 0     ## Counter for cluster files
    print(clustL)
    for aclust in clustL:
        print(aclust)
        info    = aclust.rsplit("_",4)
        print("Info:",info)
        
        if len(info)==5: ### If there is some other file or contaminant then it is not considered, need better way to filter those out
            alib    = info[0].rsplit('.',1)[0]
            pval    = info[1].replace("p","")
            aphase  = info[3]

            print(alib,pval,aphase)
            if alib in libs and pval == str(pcutoff) and str(aphase) == str(phase): ## phase added to ensure that if user runs both 21 and 24-nt PHAS in same directory, the files from different then specific phase are not picked up
                # print("--Cluster file found:%s" % (alib ))
                # print("----Filename:%s | Lib:%s | pval = %s" % (aclust,libs,pval))
                afile = "%s/%s" % (args.dir,aclust)
                combL.append(afile)
                bcount +=1
    # print("--%s cluster files captured for concatanation" % (len(combL)))

    ### Concatanate these files #######################
    ###################################################
    aname       = "%s/ALL.%sPHAS_p%s_srna.cluster" % (res_folder,aphase,pcutoff)
    clustfile   = FileCombine(combL,aname)

    ### Sanity Check ##################################
    ###################################################
    if acount == 0 or bcount == 0 :
        print("** No phaser results detected")
        print("** Check if periodicity i.e. 21nt, 22nt or 24nt from phaser analysis")
        print("   and periodicity mentioned in 'phaser.set' is same")
        sys.exit()
    else:
        pass
    print("--%s files prepared for collapsing" % (acount))

    print("--Working folder:%s | Temporary Folder:%s\n" % (res_folder,temp_folder))
    # sys.exit()

    return temp_folder,clustfile

def removeRedundant(temp_folder,p_val,fileType,overlapCutoff):
    """
    Remove redundant entries by checking those in the pool
    """
    # print(overlapCutoff)
    # sys.exit()

    print ("\n#### Fn: PHAS collapser #########################")

    fh_out = open('PHASRedundant.log','w')
    ##Read files
    if fileType == 'Y':
        fls = glob.glob(r'./%s/*.YES.by.PARE.txt' % (temp_folder))
    elif fileType == 'N':
        fls = glob.glob(r'./%s/*.YES.by.PARE.txt' % (temp_folder))
    else:
        fls = glob.glob(r'./%s/*converted.list' % (temp_folder))
        
    #print (fls,'\n')
    #print ('Total files passed for redundancy removal: %s' % (len(fls)))
    
    main_dict = {} ## declare empty dictionary
    anum = 1 ## To name PHAS loci
    for afile in fls: ###
        print ('**\nAnalyzing file: %s\n' % (afile))
        tmp_dict = {}   ## Dictionary to store values for one file - recycled after every file
        tmp_list = []   ## List to hol dall co-odinates before making a dictionary based on chromosme
        neg_list = []   ## List to store keys that needs to be removed
        fh_in = open(afile, 'r')
        alib = afile.split('/')[-1].split('.')[0] ###
        # print ("File:",afile,"lib:",alib)
        
        shutil.rmtree('./temp',ignore_errors=True)
        os.mkdir('./temp')
        outfile = './temp/PHAS_Uniq_%s' % (afile.split('/')[-1]) ### File to record unique entries of every library
        fh_out2 = open(outfile,'w')
        
        ###First Instance - Fill up the dictionary with entries from first file
        if not main_dict.values(): ## First file will populate dictionary
            lines = [i for i in fh_in if i[:-1]] ## Remove empty lines one liner and read all with content
            
            for ent in lines:
                ent_splt = ent.strip('\n').split('\t')
                
                if float(ent_splt[1]) == float(args.pval): ### Equals p-value cut-off specifed above
                    key = '%s-%s-%s' % (ent_splt[2],ent_splt[3],ent_splt[4])    ### chrid, start and end makes a key
                    
                    chrid = ent_splt[2]
                    start = int(ent_splt[3])
                    end = int(ent_splt[4]) ## 1 added because when opening a range using start and end, end number is not included in range - - Critical bug fixed in v4->v5 and later regressed/removed in v9->v10
                    
                    value = ((chrid,start,end),ent_splt,alib,key) ## Added key in value so that we don't need to make it later
                    print("Key",key,"| Value",value)
                    

                    tmp_dict[key] = value
                    

            main_dict.update(tmp_dict) ## Update the main dict

        ## Second and further Instance - Match the entries of new file with dictioary and add new one #############
        else:                           ## Dictionary has keys and their values populated from first file and now remove redundancy
            lines = [i for i in fh_in if i[:-1]] ## Remove empty lines one liner
            
            for ent in lines:
                ratiodict= {}           ## Dictonary to hold ratios of comparision of this entry with all in dictionary
                ent_splt = ent.strip('\n').split('\t')
                #print('\nCurrent entry:',ent_splt)
    
                ### Compare with dict entries and get ratio
                if float(ent_splt[1]) == float(args.pval): ### Check p-value cutoff
                    #print('Adding')
                    new_chrid = ent_splt[2] ## Chromosome or transcript name
                    new_start = int(ent_splt[3])
                    new_end = int(ent_splt[4])  ### 1 added because when opening a range using start and end, end number is not included in range - Critical bug fixed in v4->v5 and later regressed/removed in v9->v10
                    newRegion = list(range(new_start,new_end))
                    
                    ## print('matching')
                    for i in main_dict.values():    ##Compare with all dictionary values
                        #print(i)
                        existKey = i[3]
                        exist_chrid = i[0][0]
                        exist_start = i[0][1]
                        exist_end = i[0][2]

                        if new_chrid == exist_chrid: ## Check if chr or transcript is same
                            #print (i, main_dict[i])
                            existRegion = list(range(exist_start,exist_end))
                            #print (existRegion,newRegion)
                            sm=difflib.SequenceMatcher(None,existRegion,newRegion)
                            ratiodict[str(existKey)]=round(sm.ratio(),2) ### Make a dict of main dict entries and their comparision ratio with current new entry

                        else:
                            ratiodict[str(existKey)]=round(0.00,2) ## None of the existing entry matches with the current ones
                            pass
                else:
                    #print('p-value cutoff not matching')
                    continue


                ############################################################################################################################
                ## Decide if entry is different enough to be added - Get the entry with max match to one being tested and make its key again
                
                existKey = max(ratiodict,key=ratiodict.get) ## Key from main_dict with max comparable ratio for current entry
                maxratio = ratiodict[existKey]              ## Max ratio with the earlier phased locus
                print("\n\n#####################")
                print(existKey.strip(),maxratio)                    ## If maxratio is zero same entry will appear again here
                
                
                ## Key for current entry is made only if there is some match in region
                newKey = '%s-%s-%s' % (ent_splt[2],ent_splt[3],ent_splt[4])
                newValue = ((new_chrid,new_start,new_end),ent_splt,alib,newKey) 
                
                if maxratio <= overlapCutoff: ### Overlap is less then cutoff then treat as new loci and no need to delete any entry from existing values
                    #print ('Adding new key')
                    tmp_dict[newKey]=newValue ## Life = one file
                    fh_out2.write('%s\t%s\t%s\t%s\t%s\t%s\t%s\n' % (newValue[1][0],newValue[1][1],newValue[1][2],newValue[1][3],newValue[1][4],newValue[1][5],newValue[1][6])) ### Entries unique to each library - library name included in file name
    
                    
                elif maxratio > overlapCutoff: #### Choose the longest loci
                    print('Selecting longest loci')
                    print('Remade',existKey, 'New Key',newKey,)
                    achrid,astart,aend = existKey.rsplit("-",2) ## Start and end extracted from existKey because exist_start and exist_end belonged to last ent in dictionary and not the
                                                                ## one with max overlaping existKey
                    print ('Length of existing:%s | Length of new:%s' % ( (int(aend)-int(astart)+1), (int(new_end)-int(new_start)+1) ) )
                    
                    if (int(exist_end)-int(exist_start)+1) < (int(new_end)-int(new_start)+1): ### New Loci is longer ------------------------>>>>>>>>>>>>>>>>>. Check
                        print ('New phased loci is longer')
                        neg_list.append(existKey)
                        tmp_dict[newKey]=newValue
                    
                    else: ## The loci in dictionary is longer
                        print('Existing phased loci is longer or equal to new')
                        pass
                else:

                    print('Redundant')
                    pass
                
            main_dict.update(tmp_dict) ### Update the main dict
            
            ######################## Test ####################
            ##print ('Dictionary')
            #fh_test = open('keysTest','w')
            #for i in main_dict.keys():
            #    #print (i)
            #    fh_test.write('%s\n' % i)
            #print ('\nLength of dictionary: %s' % (len(main_dict)))
            #
            #
            #fh_test2 = open('NegKeytest', 'w')
            #for i in neg_list:
            #    fh_test2.write('%s\n' % i)
            #print ('Length of negative list: %s' % (len(neg_list)))
            ################################################
            
            ## Remove keys in negative list before moving to another file
            for akey in neg_list:
                print (akey)
                try:
                    del main_dict[akey]
                    print (akey, '\nKey found in main dict and is being removed')
                except KeyError:
                    print (akey, '\nKey not found')
                    pass
            # pass

        print ('\n**Number of Phased loci after %s lib: %s**\n' % (alib,len(main_dict)))
        fh_out.write('**Number of Phased loci %s lib: %s**\n' % (alib,len(main_dict)))
        fh_out2.close()
    
    print ('Number of final phased loci: %s' % (len(main_dict)))
    fh_out.write('Number of final phased loci: %s' % (len(main_dict)))
    fh_out.close()
    return main_dict

def compare(temp_folder,fileType):
    """
    compare two csv results files generated from script - Format should be same for both files 
    """
    fh_out  = open('PHASRedundant.log','w')
    fls     = glob.glob(r'./%s/*converted.list' % (temp_folder))
    
    main_dict   = {} ## declare empty dictionary
    anum        = 1 ## To name PHAS loci
    
    for afile in fls: ###
        print ('**\nAnalyzing file: %s\n' % (afile))
        tmp_dict    = {}## Dictionary to store values for one file - recycled after every file
        tmp_list    = [] ## List to hol dall co-odinates before making a dictionary based on chromosme
        neg_list    = []## List to store keys that needs to be removed
        fh_in       = open(afile, 'r')
        alib        = afile.split('/')[-1].split('.')[0]###
        # print ("File:",afile,"lib:",alib)
        
        outfile     = 'PHAS_Uniq_%s' % (afile.split('/')[-1]) ### File to record unique entries of every library
        fh_out2     = open(outfile,'w')
        
        ###  First Instance - Fill up the dictionary with entries from first file
        if not main_dict.values(): ## First file will populate dictionary
            lines       = [i for i in fh_in if i[:-1]] ## Remove empty lines one liner and read all with content
            
            for ent in lines:
                print(ent.strip('\n'))
                ent_splt = ent.strip('\n').split('\t')
                #if float(ent_splt[1]) == float(args.pval): ### Equals p-value cut-off specifed above
                key     = '%s-%s-%s' % (ent_splt[2],ent_splt[3],ent_splt[4])###Chr id, start and end makes a key
                
                chrid   = int(ent_splt[2])
                start   = int(ent_splt[3])
                end     = int(ent_splt[4]) ## 1 added because when opening a range using start and end, end number is not included in range - - Critical bug fixed in v4->v5 and later regressed/removed in v9->v10
                value   = ((chrid,start,end),ent_splt,alib,key)
                tmp_dict[key] = value
                print('Value:',value)
            main_dict.update(tmp_dict) ## Update the main dict

        ##Second and further Instance - Match the entries of new file with dictioary and add new one#############
        else: ## Dictionary has keys and value spopulated from first file and now remove redundancy
            lines = [i for i in fh_in if i[:-1]] ## Remove empty lines one liner
            for ent in lines:
                ratiodict= {}###dict to hold ratios of comaprarision of this entry with all in dictionary
                ent_splt = ent.strip('\n').split('\t')
                print('\nEntry: %s:' % (ent.strip('\n')))
                #print('\nCurrent entry:',ent_splt)
    
                ###Compare with dict entries and get ratio
                new_chrid = int(ent_splt[2])
                new_start = int(ent_splt[3])
                new_end = int(ent_splt[4])### 1 added because when oening a range using start and end, end number is not included in range - Critical bug fixed in v4->v5 and later regressed/removed in v9->v10
                value = ((chrid,start,end),ent_splt,alib)
                print('Value:',value)
                newRegion = list(range(start,end))
                
                #print('matching')
                for i in main_dict.values():##Compare with all dictionary values
                    #print(i)
                    existKey = i[3]
                    exist_chrid = i[0][0]
                    exist_start = i[0][1]
                    exist_end = i[0][2]


                    if new_chrid == exist_chrid:
                        #print (i, main_dict[i])
                        existRegion = list(range(exist_start,exist_end))
                        #print (existRegion,newRegion)
                        sm=difflib.SequenceMatcher(None,existRegion,newRegion)
                        ratiodict[str(existKey)]=round(sm.ratio(),2)### Make a dict of main dict entries and their comparision ratio with current new entry
                    else:
                        ratiodict[str(existKey)]=round(0.00,2) ## None of the existing entry matches with the current ones
                        pass

                ############################################################################################################################
                ## Decide if entry is different enough to be added - Get the entry with max match to one being tested and make its key again

                existKey = max(ratiodict,key=ratiodict.get)### Key from main_dict with max comparable                 
                maxratio = ratiodict[existKey]              ## Max ratio with the earlier phased locus
                print("\n\n#####################")
                print(existKey.strip(),maxratio)                    ## If maxratio is zero same entry will appear again here

                ## Key for current entry is made only if there is some match in region
                newKey = '%s-%s-%s' % (ent_splt[2],ent_splt[3],ent_splt[4])
                newValue = ((new_chrid,new_start,new_end),ent_splt,alib,newKey)
                
                if maxratio <= 0.20: ### Overlap is less then cutoff then treat as new loci and no need to delete any entry from existing values
                    #print ('Adding new key')
                    tmp_dict[newKey]=newValue ## Life = one file
                    fh_out2.write('%s\t%s\t%s\t%s\t%s\t%s\t%s\n' % (newValue[1][0],newValue[1][1],newValue[1][2],newValue[1][3],newValue[1][4],newValue[1][5],newValue[1][6])) ### Entries unique to each library - library name included in file name

                else:
                    print('Redundant')
                    pass
                
            main_dict.update(tmp_dict) ### Update the main dict

            ## Remove keys in negative list before moving to another file           
            for akey in neg_list:
                print (akey)
                try:
                    del main_dict[akey]
                    print (akey, 'Key found in main dict and is being removed')
                except KeyError:
                    print (akey, '\nKey not found')
                    pass
                
            # pass

        print ('\n**Number of Phased loci after %s lib: %s**\n' % (alib,len(main_dict)))
        fh_out.write('**Number of Phased loci %s lib: %s**\n' % (alib,len(main_dict)))
        fh_out2.close()
    
    print ('Number of final phased loci: %s' % (len(main_dict)))
    fh_out.write('Number of final phased loci: %s' % (len(main_dict)))
    fh_out.close()
    return main_dict

def writer_collapse(main_dict):
    '''
    Writes collapsed results
    '''
    print ("\n#### Fn: Collapsed Writer #########################")
    outfile1    = "./%s/%sPHAS_p%s_collapsed.txt" %     (res_folder,phase,args.pval)
    fh_out1     = open(outfile1,'w')
    outfile2    = "./%s/%sPHAS_p%s_collapsed.list" %    (res_folder,phase,args.pval)
    fh_out2     = open(outfile2,'w')### For our Genome viewer, No header required
    # outfile3    = "./%s/Final_PHASLociID_%s_%s.ping" % (res_folder,args.pval,res_folder)
    # fh_out3     = open(outfile3,'w')
    
    if fileType == 'C':
        fh_out1.write('Name\tp-val\tChr\tStart\tEnd\tStrand\tLib\tfileName\n')
        for value in main_dict.values():
            print(value)
            #print('Phas-%s\t%s\t%s\t%s\t%s\t%s\n' % (anum,value[1][1],value[1][2],value[1][3],value[1][4],value[1][6]))
            fh_out1.write('%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n' % (value[1][0],value[1][1],value[1][2],value[1][3],value[1][4],value[1][5],value[1][6],value[2]))
            fh_out2.write('%s.%s.%s\n' % (value[1][2],value[1][3],value[1][4]))
            # fh_out3.write('%s\t%s\t%s\t%s\t%s\t%s\t%s\n' % (value[1][0],value[1][1],value[1][2],value[1][3],value[1][4],value[1][5],value[1][6]))
        
    else:
        fh_out1.write('Name\tp-val\tChr\tStart\tEnd\tStrand\tLib\n')
        anum = 1
        for value in main_dict.values():
            # print(value)
            #print('Phas-%s\t%s\t%s\t%s\t%s\t%s\n' % (anum,value[1][1],value[1][2],value[1][3],value[1][4],value[1][6]))
            fh_out1.write('Phas-%s\t%s\t%s\t%s\t%s\t%s\t%s\n' % (anum,value[1][1],value[1][2],value[1][3],value[1][4],value[1][6],value[2]))
            fh_out2.write('%s.%s.%s\n' % (value[1][2],value[1][3],value[1][4]))
            # fh_out3.write('%s\t%s\t%s\t%s\t%s\t%s\t%s\n' % (value[1][0],value[1][1],value[1][2],value[1][3],value[1][4],value[1][5],value[1][6]))
            anum +=1
            
    fh_out1.close()
    fh_out2.close()
    # fh_out3.close()
    return outfile1,outfile2

def listConverter(temp_folder,fileType):
    
    print("\n#### Fn: listconverter ############################")
    print(temp_folder)

    if fileType == 'L':
        print('\nList files selected for analysis - Converting them to readable format')
        fls = glob.glob(r'./%s/*.PARE.validation.list' % (temp_folder))
        print ('\nHere are the files that will be converted:',fls,'\n')
        print ('Total files to analyze: %s' % (len(fls)))
        for fl in fls:
            fh_in = open(fl,'r')
            fh_out = open('%s_converted.list' % (fl.rpartition('.')[0]),'w')
            entries = fh_in.readlines()
            for i in entries:
                if i.strip(): ## Remove an empty line from end file
                    ent_splt = i.strip('\n').split('=')
                    #print(ent_splt[0].split('|'))
                    pval,phase,trash = ent_splt[0].strip().split('|')
                    chromo_start,end = ent_splt[1].strip().split('..')
                    chromo,sep,start = chromo_start.rpartition(':')

                    if runType      == 'Y':
                        fh_out.write('%s\t%s\t%s\t%s\t%s\tNONE\tNONE\n' % (phase,pval,chromo.strip().replace('chr',''),start,end)) ##Chromosome has space before it which later gives error while key matching
                    elif runType    == 'N': ## Header has lots of stuff
                        fh_out.write('%s\t%s\t%s\t%s\t%s\tNONE\tNONE\n' % (phase,pval,chromo.strip(),start,end)) ##Chromosome has space before it which later gives error while key matching

            fh_in.close()
            fh_out.close()
        
    if fileType == 'C':
        print('\nList files selected for analysis - Converting them to readable format')
        fls = glob.glob(r'./%s/*.csv' % (temp_folder))
        print ('\nHere are the files that will be converted:',fls,'\n')
        print ('Total files to analyze: %s' % (len(fls)))
        for fl in fls:
            fh_in = open(fl,'r')
            header = fh_in.readline()
            fh_out = open('%s_converted.list' % (fl),'w')
            entries = fh_in.readlines()
            for i in entries:
                if i.strip(): ## Remove an empty line from end file
                    ent_splt = i.strip('\n').split('\t')
                    #print(ent_splt[0].split('|'))
                    name,pval,chromo,start,end,strand,Lib = i.strip('\n').split('\t')
                    fh_out.write('%s\t%s\t%s\t%s\t%s\t%s\t%s\n' % (name,pval,chromo,start,end,strand,Lib))##Chromosome has space before it which later gives error while key matching
            fh_in.close()
            fh_out.close()
    else:
        pass
    
    return None

#### SUMMARIZER #################################
#################################################

def PHASreader(coordsfile):
    '''
    Reads coordinates file and return a list of PHASE ID and their coords values
    '''

    print("\n#### Fn: PHAS Reader #################")

    fh_in = open(coordsfile)
    if head == 'Y':
        phashead = fh_in.readline()

    phasCount   = 0       ## Count of entries read
    phasList    = []       ## List to store results 
    for ent in fh_in:

        # print('\n\nEntry from input file being matched: %s' % (ent))
        coords = ent.split(coordsep)
        #print('coords:',coords)
        if phasedID == 'Y':                                                         ## Extract coords from phased ID
            loci = coords[0].split('_')                                             ## Phas-10_w_3:27574117:27574772
            # phasID = loci[0]
            fusedcoords     = loci[2].split(':')                                    ## 3:27574117:27574772
            aname           = coords[namecol-1]
            apval           = coords[pvalcol-1]
            alib            = coords[libcol-1].strip('\n')
            get_chr_id      = fusedcoords[0].replace("chr","").replace("Chr","")
            astart          = fusedcoords[1]
            aend            = fusedcoords[2]
            get_start       = int(astart)+startbuff                         ## It should be added to reduce length of loci
            get_end         = int(aend)+1 
            phasID          = '%s_%s_%s' % (get_chr_id,astart,aend)           ## 1 added because when opening a range using start and end, end number is not included in range
            phasCount       += 1
        
        elif phasedID == 'N': ## User specified coords
            # print("File with user specified columns will be used")
            aname           = coords[namecol-1]
            apval           = coords[pvalcol-1]
            alib            = coords[libcol-1].strip('\n')
            get_chr_id      = coords[chrcol-1]
            astart          = coords[startcol-1]
            aend            = coords[endcol-1] 
            get_start       = int(int(astart)+startbuff)
            get_end         = int(aend)+1                                       ## 1 added because when opening a range using start and end, end number is not included in range
            phasID          = '%s_%s_%s' % (get_chr_id,astart,aend)             ## Name to be used in output file
            print("Phased Loci: %s #######################################################" % (phasID))
            phasCount       += 1

        else:
            print("Please input correct value for 'phasedID' or check your file")

        get_value  = (list(range(int(str(get_start)),int(str(get_end)))))
        phasList.append((aname,apval,alib,phasID,get_chr_id,get_start,get_end,get_value))
        
    print("Entries read:%s | Entries cached:%s" % (phasCount,len(phasList)))

    return phasList,phashead

# def prepareDicts(libs):
    # '''
    # prepares tag count dictionaries by using readFileToDict
    # '''

    # print("\n#### Fn: prepareDics #################")
    
    ## Initialize the result dictionary list for results
    # dictList    = []

    # # Utilize a code for the dictionary to identify the 
    # indexList   = list(range(len(libs)))
    # dictList    = PPResults(readFileToDict, indexList)

    # return dictList

def getClust(clustfile,phasList):

    print ("\n#### Fn: Cluster Search #########################")
    
    fh_in           = open(clustfile,'r')
    clusters        = fh_in.read().split('>')
    
    resList         = [] ## Store final results as (phas,[(phasiRNA),(PhasiRNA)],[extra info])
    resList2        = [] ## Store phasiRNAs from all clusters as (phas,[(phasiRNA),(PhasiRNA)],[extra info])
    
    phasCount       = 0                                                          ## Total phased loci in file
    uniqMatchCount  = 0                                                          ## Atleast one cluster present for one phased loci
    allMatchCount   = 0                                                          ## Total number of matched cluster
                                                            
    for ent in phasList:                                                           ## Given an entry in coords file
        # print(ent)
        aname,apval,alib,phasID,get_chr_id,get_start,get_end,get_value = ent
        # print("This is the PhasId: %s | values:%s" % (phasID,get_value))
        print("\n\nPhaseID being queried:%s ##############" % (phasID))
        phasCount +=1 
        print("%s/%s phasID" % (phasCount,len(phasList)))

        ### Find matching cluster
        matchCount          = 0        ## Total maching clusters for a phased loci - if same cluster in multiple libraries
        finalMatchList      = []       ## Holds best cluster, from multiple libraries
        tempAllList         = [] ## Hold phasiRNAs from all matching clusters, of use for phased transcripts to capture allphasiRNAs, must be used with low matchThres
        for aclust in clusters[1:]:
            tempMatchList   = [] ## To hold results of current matching cluster
            aclust_splt     = aclust.split('\n')
            header          = aclust_splt[0].split()
            clust_id        = header[2]
            chr_id          = header[6].replace("chr","").replace("Chr","")
            start           = header[10]
            end             = int(header[12])+1 ##1 added because when opening a range using start and end, end number is not included in range
            value           = (list(range(int(str(start)),int(str(end)))))
            # print ('Cluster:', (value))

            if runType == 'Y':                 ## Normal genomic coordinates with integer chr_id
                # print(chr_id)
                get_chr_id  = str(get_chr_id).replace("chr","").replace("Chr","")
                get_chr_id  = int(get_chr_id)
                chr_id      = int(chr_id)
            else:
                ## Chromosomes are transcript names  i.e. strings
                pass

            
            if get_chr_id == chr_id:
                sm          =   difflib.SequenceMatcher(None,get_value,value) ## The rratio corresponds to larger loci i.e. match/length of longer nucleotides
                # print("Get Value:",get_value)
                # print("Current value",value)
                aratio      = round(sm.ratio(),2)
                # print("Ratio:%s" % (aratio))
                
                if round(sm.ratio(),2) >= matchThres:
                    ### Matched - phasiRNA from this cluster
                    print ('\nMatching cluster found:%s' % ''.join(header))
                    print("Allowed Ratio:%s | Current Ratio:%s" % (matchThres,aratio))
                    matchCount  +=1
                    
                    phasiCyc    = 0    ## Stores phasing cycles
                    phasiSig    = 0    ## Stores total abundance of phase size sRNAs
                    otherSig    = 0    ## Stores total abundance of other size sRNAs
                    kvalsL      = []   ## List to store kvalues for each phasiRNAs
                    
                    for i in aclust_splt[1:-1]:## Because header was the first entry of block and not required here, Last entry is always empty
                        # print ("Matched Cluster:\n",i)
                        phasient    = i.split('\t')
                        phasiname   = phasient[4].replace("|","_")
                        phasiseq    = phasient[5]
                        phasilen    = int(phasient[6])
                        phasiabun   = int(phasient[7])
                        phasihits   = int(phasient[10].split("=")[1])
                        phasipos    = int(phasient[3])
                        phasistrand = phasient[2].translate(str.maketrans("+-","wc"))
                        phasipval   = phasient[12]
                        phasikval   = int(phasient[9].split('=')[1])
                        # print(phasipos)
                        # sys.exit()

                        # print(phasiname,phasiabun,phasiseq,phasilen)
                        kvalsL.append(phasikval)
                        tempMatchList.append((phasiname,phasiabun,phasiseq,phasilen,phasihits,phasipos,phasistrand,phasipval))
                        tempAllList.append((phasiname,phasiabun,phasiseq,phasilen,phasihits,phasipos,phasistrand,phasipval)) ## Records all phasiRNAs from all clusters

                        if int(phasilen) == phase:
                            phasiCyc +=1
                            phasiSig += phasiabun
                        else:
                            otherSig += phasiabun
                    sizeRatio   = round(phasiSig/(phasiSig+otherSig),2) 
                    bestkval    = max(kvalsL) ## Best k-value achieved by this cluster
                    # print("Current Cycles:%s | Current sig. strength:%s" % (phasiCyc,phasiSig))
                    print("Current Cycles:%s | Current sig. strength:%s" % (bestkval,phasiSig))
                    tempMatchList.append((bestkval,phasiSig,phasID,clust_id,sizeRatio))

                    ## Decide the best and remove other from list ##############################
                    ############################################################################
                    if finalMatchList:
                        ## There exists a previosly matched cluster
                        exist_bestkval = finalMatchList[-1][0]
                        exist_phasiSig = finalMatchList[-1][1]
                        print("Existing Cycles:%s | Existing sig. strength:%s" % (exist_bestkval,exist_phasiSig))

                        if bestkval > exist_bestkval: ## New cluster has more cycles
                            del finalMatchList[0:]
                            finalMatchList = list(tempMatchList)
                            print("--- New cluster selected ---")

                        elif bestkval == exist_bestkval: ## Both have same cycles
                            if phasiSig > exist_phasiSig: ## New one has more total abundance of phased siRNAs
                                del finalMatchList[0:]
                                finalMatchList = list(tempMatchList)
                                print("--- New cluster selected ---")
                        
                        else: ## Existing/old one was long i.e. had more cycles
                            print("Earlier recorded cluster is retained")
                            pass

                    else: ## This is the first cluster
                        finalMatchList  = list(tempMatchList)
                        allphasiList    = list(tempMatchList) 

                    # print("\nFinal Match List:",finalMatchList)
            else:
                # print("No Match with this cluster")
                pass

        tempAllList.append((bestkval,phasiSig,phasID,clust_id,sizeRatio)) ## This list has phasiRNAs from all clusters but to keep the structure same as original resList, helpful while writing results, this info is added

        phasinfo = [aname,apval,get_chr_id,get_start,get_end,alib]
        resList2.append((phasID,tempAllList,phasinfo))
        resList.append((phasID,finalMatchList,phasinfo)) ## Add best matched cluster entry to the final list, there has to be one best matched entry per PHAS
        
        allMatchCount += matchCount ## Add the matched cluster for each entry
        if matchCount > 0 :
            uniqMatchCount+=1
        
    print("\nTotal phas loci: %s | Matched: %s" % (len(phasList),len(resList)))
    print ("SUMMARY: Phased loci in input file:%s | Loci match threshold: %s |Uniq matched cluster: %s | Total matched clusters found:%s" % (phasCount,matchThres,uniqMatchCount,allMatchCount))
    print("NOTE: If matched clusters more then phased loci that means same cluster was present in different libs\n")
    # print("NOTE: Don't forget to uniq the miRNAs")
    fh_in.close()

    return resList,resList2

def allphasiWriter(clustfile,resList):
    '''
    This takes all the phasiRNAs on a transcript and return unique phasiRNAs file from all clusters - You need thos file for study of precursors
    '''

    print("\n\nFunction:allphasiWriter\n")

    outfile         = "%s_thres%s_allphasi.csv" % (clustfile.rpartition(".")[0],matchThres)
    # outfile2        = outfile = clustfile+'thres_%s_allphasi.csv' % matchThres                       ## 2437.txt.score_p1e-07_sRNA_21_out.cluster
    fh_out          = open(outfile2,'w')

    ## Find entries with unique seq
    for ent in resList: ## entry corresponds to one phased loci
        # print("\nEntry",ent)
        phasID      = ent[0]
        phasCycles  = ent[1][-1][0]
        phasSig     = ent[1][-1][1]
        clustID     = ent[1][-1][2]
        sizeRatio   = ent[1][-1][4]
        phasiList   = ent[1][0:-1]
        phasiList_s = sorted(phasiList,key=itemgetter(5)) ## Sort on position so that results from multiple clusters are at-least sorted in postions
        # print("Sorted all phasiRNAs:", phasiList_s)
        # sys.exit()
        # print("%s | phasCycles:%s | phasSig:%s" % (phasID,phasCycles,phasSig))

        tagset = set() ## To record only the uniq tags, in case of lilium many a times same tag is documented twice
        for i in phasiList:
            print("-Phasi",i)  ## Phasiname,phasiabun,phasiseq,phasilen,phasihits,phasipos
            tag = i[2]
            
            if tag not in tagset: ## Avoid recordinf same tag agains as observed in casne on lilium
                tagset.add(tag)
                
                ## Write phasiRNAs 
                if phasiLenFilter == 'Y': ## If tags filter is ON
                    if (int(i[3]) == int(phase)) and (int(i[1]) >= minAbun): ### Size specified in settings
                        # print(phasID,clustID,i[0],i[1],i[2])
                        fh_out.write('>%s_Clust%s_%s,%s,%s,%s,%s,%s,%s\n%s,%s,%s,%s,%s,%s,%s\n' % (phasID,clustID,i[0],i[1],i[4],i[5],i[3],i[6],i[7],tag,i[1],i[4],i[5],i[3],i[6],i[7]) )  ## phasID,clust_id,phasiname,phasiabun,phasihits,phasiseq,phasiabun,phasihits,phasipos,phasistrand,phasipval
                        tagset.add(tag)

                else:
                    if int(i[1]) > minAbun:
                        # print(phasID,clustID,i[0],i[1],i[2])
                        fh_out.write('>%s_Clust%s_%s,%s,%s,%s,%s,%s,%s\n%s%s,%s,%s,%s,%s,%s\n' % (phasID,clustID,i[0],i[1],i[4],i[5],i[3],i[6],i[7],tag,i[1],i[4],i[5],i[3],i[6],i[7]) )  ## phasID,clust_id,phasiname,phasiabun,phasihits,phasiseq,phasiabun,phasihits,phasipos,phasistrand,phasipval
                        tagset.add(tag)
                        pass


    fh_out.close()
    print("\n\nExiting :allphasiWriter\n")


    return outfile2

def writer_summ(clustfile,resList,dictList):
    '''
    write the results
    '''
    print("\n#### Fn:Writer_summ #################")

    outfile = "%s_thres%s_phasi.csv" % (clustfile.rpartition(".")[0],matchThres)
    # print(outfile)
    # outfile = clustfile+'thres_%s_phasi.csv' % matchThres                       ## 2437.txt.score_p1e-07_sRNA_21_out.cluster
    fh_out  = open(outfile,'w')

    if fetchMax == 1: ### Fetch max phasi for each loci 
        outfile2    = "./%s/%sPHAS_p%s_summary.txt" % (res_folder,phase,args.pval)
        fh_out2     = open(outfile2,'w')

        libsHead    = libs
        fh_out2.write("Name\tp-val\tChr\tStart\tEnd\tbestLib\tcluster\tbestKval\tphasiDominance\tmaxTagRatio\t%s\ttotalPhasAbun\tmaxtag\tmaxTagAbun\tmaxtag2\tmaxTagAbun2\n" % ('\t'.join(x for x in libsHead)))
        # print(queryLibs) ## Libs whose abindance will be summed to give final abundance of tags
        abunList    = [] ### List to capture tags and abundance for each phased loci
        libAbunList = [] ## Lib-wise abundances of tag

    print("Writing:")
    print("--PhasiRNAs extended CSV file")
    print("--Summary file with lib-specific abundances")
    for ent in resList: ## entry corresponds to one phased loci
        # print("\nEntry",ent)
        phasID      = ent[0]
        phasCycles  = ent[1][-1][0]
        phasSig     = ent[1][-1][1]
        clustID     = ent[1][-1][2]
        sizeRatio   = ent[1][-1][4]
        phasiList   = ent[1][0:-1]
        phasinfo    = ent[2] ### The PHAs coordinates from phaser
        # print("phasinfo:",phasinfo)
        # sys.exit()
        # print("%s | phasCycles:%s | phasSig:%s" % (phasID,phasCycles,phasSig))


        tagset = set() ## To record only uniq tags, in case of lilium many a times same tag is documented twice
        for i in phasiList:
            # print("-Phasi",i)  ## Phasiname,phasiabun,phasiseq,phasilen,phasihits,phasipos
            tag = i[2]
            
            if tag not in tagset: ## Avoid recording same tag agains as observed in casne on lilium
                tagset.add(tag)
                
                ## Write phasiRNAs 
                if phasiLenFilter == 'Y': ## If tags filter is ON
                    if (int(i[3]) == int(phase)) and (int(i[1]) >= minAbun): ### Size specified in settings
                        # print(phasID,clustID,i[0],i[1],i[2])
                        fh_out.write('>%s_Clust%s_%s,%s,%s,%s,%s,%s,%s\n%s,%s,%s,%s,%s,%s,%s\n' % (phasID,clustID,i[0],i[1],i[4],i[5],i[3],i[6],i[7],tag,i[1],i[4],i[5],i[3],i[6],i[7]) )  ## phasID,clust_id,phasiname,phasiabun,phasihits,phasiseq,phasiabun,phasihits,phasipos,phasistrand,phasipval
                        tagset.add(tag)

                else:
                    if int(i[1]) > minAbun:
                        # print(phasID,clustID,i[0],i[1],i[2])
                        fh_out.write('>%s_Clust%s_%s,%s,%s,%s,%s,%s,%s\n%s%s,%s,%s,%s,%s,%s\n' % (phasID,clustID,i[0],i[1],i[4],i[5],i[3],i[6],i[7],tag,i[1],i[4],i[5],i[3],i[6],i[7]) )  ## phasID,clust_id,phasiname,phasiabun,phasihits,phasiseq,phasiabun,phasihits,phasipos,phasistrand,phasipval
                        tagset.add(tag)
                        pass


                ## Get max abundance phasiRNA for specified phase
                if fetchMax == 1:

                    ## Get lib-wise abundaces mode
                    if fetchLibAbun == 0:
                        if len(tag) == int(phase):
                            atag,abun_sum,lib_abun = getAbundanceLocal(tag,dictList)
                            libAbunList.append((lib_abun))

                    elif fetchLibAbun == 1: ## All the tags
                        atag,abun_sum,lib_abun = getAbundanceLocal(tag,dictList)
                        libAbunList.append((lib_abun))
                    
                    else:
                        print("Libwise abundances won't be fetched")
                        pass

                    ## Tag specific abundances for fetching most abundant tag
                    if len(tag) == int(phase): ### Size specified in settings
                            abunList.append((atag,abun_sum))
            else:
                print("Tag recorded once already#####################################\n")
                # sys.exit()
                pass


        # print("Elements in phasiList:%s" % (len(phasiList)))
        # print("Elements in libAbunList:%s" % (len(libAbunList)))


        ## Process Fetch max results for this phased loci
        #################
        if fetchMax == 1:
            
            abunList_sort = sorted(abunList, key=operator.itemgetter(1),reverse=True) ## Sort list on abundances to get max abundant phasiRNA
            # print("\nExample sorted values:%s" % (abunList_sort[0:10]))
            maxTag      = abunList_sort[0][0]   ## Max abundant phasiRNA sequence
            maxAbun     = abunList_sort[0][1]   ## Abundance of max abundant phasiRNA
            if len(abunList_sort) > 1: ## In one case no second tag was found
                maxTag2     = abunList_sort[1][0]   ## Max abundant phasiRNA sequence
                maxAbun2    = abunList_sort[1][1]   ## Abundance of max abundant phasiRNA
            else:
                maxTag2     = "na"
                maxAbun2    = "0"
                print("The prediction is of really low quality - Just one tag of phase size found")
                time.sleep(2)
                # sys.exit()
            totalAbun   = sum(int(i[1]) for i in abunList_sort) ## Phased abundance
            maxTagRatio = round(maxAbun/totalAbun,2)


            ## Sum Lib-wise abundances for all tags
            libAbunSum = [0]*len(libAbunList[0]) ## This will hold sum of all tags, intialized for number of libraries
            for tag in libAbunList:
                # print(tag)
                libAbunSum  = [sum(x) for x in zip(libAbunSum,tag)]
            
            # print("Tag:%s | maxPhasTag:%s | totalPhasAbun:%s" % (maxTag,maxAbun,totalAbun))
            # print("Libwise Abundances",libAbunSum)

            ## Write - Loci, most abundant tag, most abundant tag abun,total phased abun, number of phased tags, and lib-wise abundances
            fh_out2.write("%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\n" % ('\t'.join(str(x) for x in phasinfo),phasID,str(phasCycles),sizeRatio,maxTagRatio,'\t'.join(str(x) for x in libAbunSum),totalAbun,maxTag,maxAbun,maxTag2,maxAbun2))
            abunList    = [] ## Empty before next phased loci
            libAbunList = [] ## Empty lib-iwse abundances of tag list before next entry
            # sys.exit()

    fh_out.close()
    fh_out2.close()

    return outfile,outfile2

def getAbundanceLocal(tag,dictList):
    """The core function for getting the abunance from a single tag
    from a series of libraries provided in tag count format. Makes
    the calls to create the data structures to get the abundance of
    a tag across several libraries

    Args:
        tag: Sequence to search the libraries for
        tagCountFilenamesList: The names of all libraries
    Returns:
        The input tag, abundance in all libraries, and the sum of 
        abundance across all libraries. 
    
    """
    # Initialize a list for the abundance results
    tagAbunList = []
    # Search for the tag in all libraries
    for dict in dictList:
        tagAbunList.append(getAbunFromDict(dict, tag))

    # The dictionaries may not be sorted in the given order, so sort them
    # according to the key (that being the index returned by getAbunFromDict)
    print("\nTag:",tag)
    print("Unsorted:",tagAbunList)
    sortedTagAbunList = sorted(tagAbunList, key=operator.itemgetter(0),
        reverse = False)
    print("Sorted:",sortedTagAbunList)

    # Store just the abundances for return
    abunList = []
    for abun in sortedTagAbunList:
        abunList.append(abun[1])

    return(tag,sum(abunList),abunList)

def getAbundance(cur,tag,finalLibs):
    '''Input is tag for each loci and out put is tag with maximmum abumdance and sum of phasiRNAs - 
    rewritten in v1.0 for fetching tags from run master'''

    lib_abun = [] ## list to hold lib-wise abudnances
    
    for alib in finalLibs:
        # print("Lib:",alib)
    
        cur.execute("SELECT tag,norm FROM %s.run_master where tag = '%s' and lib_id = %s" % (db,tag,alib))### Convert intergenic to gene name so as to get strand
        info = cur.fetchall() ## Entries are redundant, one for every hit
        # print("Query fetched", info)

        if info:
            atag,norm_abun = info[0]
            lib_abun.append(norm_abun)
            # print("--Tag abundance:%s for lib:%s"% (tag,norm_abun))
        else:
            norm_abun = 0
            lib_abun.append(norm_abun)
            # print("--Tag abundance:%s for lib:%s"% (tag,norm_abun))


    abun_sum = sum(lib_abun)
    # print("--Lib-wise abundances",lib_abun)
    # print("--Sum of abundances:%s\n" % (abun_sum))

    # sys.exit()

    return tag,abun_sum,lib_abun
            
def prepareQuery(excludeLibs,cur):

    ### Prepare query of libs #################

    ### Get lib names
    columns = [] ## empty list
    cur.execute("SELECT DISTINCT(lib_id) FROM %s.run_master" % (db))
    info = cur.fetchall()
    libs = [x[0] for x in info]

    print("\nLibs:",libs)

    if excludeLibs:
        print("\n\nLibs specifed in excludeLibs %s will be skipped\n\n" % (excludeLibs))
        selectLibs      = [] ## Columns excluding the unwanted libraries
        excludeLibs_s   = [str(i) for i in excludeLibs] ## Converting all entries in exclude list to string for matching below
        
        for i in libs:

            ## Check if user made mistake in libType - as that will give results for all entries
            if type(i) is int and libType == 1:
                print("You seem to have input lib_id and chosen wrong libType")
                print("Check libType and excludeLibs match - Script will exit now")
                sys.exit()
            elif type(i) is str and libType == 0:
                print("You seem to have input lib_id and chosen wrong libType")
                print("Check libType and excludeLibs match - Script will exit now")
                sys.exit()
            else:
                print("All seems well")
                pass

            ### Filter libraries
            if str(i) not in excludeLibs_s: ## Tested OK
                selectLibs.append(i)
            else:
                print("excluded:",i)
                # sys.exit()
                pass
        
        finalLibs = selectLibs ## Norm_Sum and max_norm are not included

    else:
        finalLibs = libs ## ## Norm_Sum and max_norm are not included

    # print("finalLibs:%s" % (finalLibs))
    
    lib_col =",".join(str(x) for x in finalLibs)### Manually mentioned strating lib column - should work on all tag position summary tables
    
    print("\nLibrary Columns:",lib_col)
    # queryLibs = 'SUM(%s)' % lib_col.replace(",","),SUM(")
    sumLibs = "%s" % lib_col.replace(",","+")
    queryLibs = "%s" % lib_col.replace(",",",")
    print("\nThese are sumLibs:",sumLibs)
    print("\nThis is query Libs:",queryLibs)
    # sys.exit()

    return queryLibs,sumLibs,finalLibs           

def FileCombine(alist,aname):

    # print ('--Files to concatanate:',alist)
    fh_out = open(aname ,'w')
    
    for x in alist:
        print ("--Concatanating:%s"% (x))
        afile   = open('./%s' % (x), 'r')
        data    = afile.read()
        afile.close()
        fh_out.write(data)
    
    fh_out.close()
        
    return aname

def cleaner():
    '''
    Picks and cleans unwanted files
    '''
    print ("\n#### Garbage Cleaner #########################")
    garbage = [afile for afile in os.listdir('%s/' % (res_folder)) if afile.endswith (('.cluster','.zip'))] 
    print("Garbage:",garbage)

    for afile in garbage:
        apath =   "%s/%s" % (res_folder,afile)  
        if os.path.isfile(apath): ## Check to see its a file from bowtie and not tophat mapped folder - Untested
            print("Deleting:%s" % (apath))
            os.remove(apath)
        else:
            # print("Skipping cleanup, as its a directory %s" % (afile))
            pass

    return None

def readFileToDict(index):
    """This function simply reads all tags in a tag count file
       and stores them in a dictionary

    Args:
        filename: The name of an individual file to read
        index: A numberical index for the order that the results should
               eventually be provided
    Returns:
        Dictionary of the read file

    """

    filename = libs[index]
    print("Caching %s to fetch phasiRNA abundances" % (filename))

    # Initialize an empty dictionary for the results
    # Use the index as a key to the dictionary so that the file
    # can be associated with this dictionary later
    fileDict = {index: {}}

    # Read through the file and store the tag count file into 
    # the dictionary
    with open(filename) as f:
        for line in f:
            # tag and abundance should be separated by a tab. Strip
            # the new line at the end of each line
            tag, abun = line.strip('\n').split('\t')

            # Remove any white space that is inadvertently left
            tag = tag.strip()
            abun = abun.strip()

            # Store the 
            fileDict[index][tag] = int(abun)

    f.closed

    return(fileDict)

def getAbunFromDict(dict, tag):
    """Get the abundance of the tag from a dictionary

    Args:
        tag: The tag to be queried from the dictionary
        dict: The dictionary of tags and their abundances in ech library
        libList: The list of libraries as provided by the user
    Returns:
        Three separate elements. The first is the tag, the second
        is the summed abundance across all libraries, and the final
        is a list of the individual library abundances

    """

    # Pull the library index from the dictionary
    index = list(dict.keys())[0]

    # Try to abundance of tag. If it doesn't exist in this library,
    # return 0 for the abundance
    try:
        return([index, dict[index][tag]])

    # Tag doesn't exist in this dictionary so return 0
    except KeyError:
        return([index,0])

def PPResults(module,alist):
    npool = Pool(int(nproc))
    res = npool.map_async(module, alist)
    results = (res.get())
    npool.close()

    return results

#### MAIN #######################################
#################################################
def main():

    ### Collapser #########################################
    libs    = readSet(setFile)
    pcutoff = pvaluereader()
    temp_folder,clustfile = prepare(pcutoff,libs,res_folder)
    if region == 'G':
        # global overlapCutoff
        overlapCutoff = 0.20 ## = 0.25 for genomic and 0.50 for ncRNAs
    else:
        # global overlapCutoff
        overlapCutoff = 0.25 ## = 0.25 for genomic and 0.50 for ncRNAs

    if fileType == 'L':
        listConverter(temp_folder,fileType)
        main_dict = removeRedundant(temp_folder,args.pval,fileType,overlapCutoff)
    elif fileType == 'C':
        listConverter(temp_folder,fileType)
        main_dict = compare(temp_folder,fileType)
    else:
        main_dict = removeRedundant(temp_folder,args.pval,fileType,overlapCutoff)
    
    outfile1,outfile2 = writer_collapse(main_dict)


    ### Summarizer ###########################################
    
    ## Read phasiFile
    phasList,phashead = PHASreader(outfile1)
    time.sleep(1)

    ## Get the clusters
    resList,resList2 = getClust(clustfile,phasList)
    if runType == 'N':
        allphasiFile = allphasiWriter(clustfile,resList2)
    else:
        print("File with all PHAS will not be generated for this 'runType'")
        pass

    ## Prepare dictionary of tag count files for abundance queries
    dictList    = []
    indexList   = list(range(len(libs)))
    dictList    = PPResults(readFileToDict, indexList)

    ## Write the summary
    results,results2 = writer_summ(clustfile,resList,dictList)

    ### Clean unwanted files
    if cleanup == 1:
        cleaner()

if __name__ == '__main__': 
    if cores == 0 :
        nproc = int(multiprocessing.cpu_count()*0.90)
    else:## As mannually entered by the user
        nproc = int(cores)
    start = time.time()
    main()
    print ("\n#### Summarization took", round(time.time()-start,2),"seconds")
    print ("#### Please see '%s' folder for results\n" % (res_folder))
    sys.exit()


#### Change Log ################################
################################################

## v01 -> v03
##Added functionality to compare between similar loci and select the longest one

## v03 -> v04 (Critical bug fixed)
## Corrected bug with 'key' - Range was being made one integer less as last integer is never counted and with every run (file) there is change in key (one integer less) it is
###Therefore key from first run not matched woth earlier and all loci being retained

## v04-> v05 (Critical bug fixed)
#####Bug with main key remade fixed

## v05 -> v06
### 1. If a lower cutoff is used than only entries pertaining to that cutoff should be analyzed, as phas are redundant between different cutoffs
###Problem observed was that 'Key not found' and reason for that is because Key has already been removed during first instance and later
###Instances of same loci but lower cutoff find main key missing from main dictionary thats why error.
###Only entries for input cutoff will be analyzed
### 2. capture loci that are different in two given files

## v06 -> V07
###Pingchuan format added to compare files from different tissues
###Added library name in log file
###Added new files for each librray with clusters added - good to identify unique clusters i.e PARE validated and non-valiadted

###V08 -> v09 (Feb 2014)
##Added functionality to work on three file types and distinguish on the basis of switch - Working OK

## v09 -> v10 - Critical speed bug fixed
### The bug was related to appending chr to the start and end abd comparing the range. In cases when there is change in number of digits between start and end. The range for matching was generated incorrectly long and leads to freezing of script.
## In this version chromosome is not appended to start and end instead a check is doen for same chromosme and than range is generated, folllowed bu comparing ranges for overlap
## REgression - In this version I was getting error related to key-not found which is kind of historical error. Maybe in past (v04 -> v05) it was addressed by adding 1 to the end co-ordinates as while opening a range last number is not counted
## So after remaking the mainkey the last number as absent is not included in the key or something like that - For future 'Key not found' errors please check this first that
## Added p-val in name

## v1.0 -> v1.01
## Introduced functionality to combine results from csv files - this will help comparing old files with new files to identify new phased loci that needs manual curation
## Removed header as output of list converter as it's not generated always

## v1.0 -> v1.02
## Added overLap ratio for different regions in main()
## Removed chr_id as integer type as ncRNA names are not integers
## Modified main_key remade with additional replace of replace("'","") to make it functional for string chromosome/transcript IDs

## v1.03 -> 1.03
## Added a temp folder to write all "Uniq" files not working directory is not filled up with all less used files

## v1.03 -> v1.04
## Changed the RemoveRedundant module to be compatible with transcripts with colplex header like PacBio
## Changed the RemoveRedundant module so that no mainkey needs to be remade, instead it's stored as last value 

## v1.04 -> v1.05 [stable]
## "compare" module needs updated with changes made in v1.04 

## v1.06 -> v1.06 [stable]
## Fixed a syntax waring about gloabal variable 'overlapCutOff' declaration

## v1.06 -> collapser
## Find matching p-value less then user specified, select the highest size file name, extract uniq p-values, abd identify the one below specified cutoff
## Read settings file to get the library names, make working directory, and provide name of this folder (collapsed)
## Command line for cutoff with recomended cutoff

## v1.09 -> v1.10
## Added Reza modules to fetch library wise abundances
## Tested Rzea module using the tagcount files downloaded from server with norm abudnaces and they match with the internal version

## Script generates an error at final exit() in __main__
## Error in atexit._run_exitfuncs:
## Traceback (most recent call last):
##  File "/usr/local/lib/python3.3/multiprocessing/forking.py", line 145, in terminate
##    os.kill(self.pid, signal.SIGTERM)
## ProcessLookupError: [Errno 3] No such process

